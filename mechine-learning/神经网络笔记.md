# 神经网络笔记

## 1. 神经网络基础

### 1.1 什么是神经网络
- 受生物神经系统启发的计算模型
- 由大量人工神经元相互连接组成
- 能通过数据自动学习特征和模式

### 1.2 神经元模型
- 输入：$x_1, x_2, ..., x_n$
- 权重：$w_1, w_2, ..., w_n$
- 加权和：$z = \sum_{i=1}^{n} w_i x_i + b$（b为偏置）
- 激活函数：$a = f(z)$

### 1.3 常用激活函数
- Sigmoid：$f(z) = \frac{1}{1+e^{-z}}$，输出范围(0,1)
- ReLU：$f(z) = \max(0, z)$，缓解梯度消失
- Tanh：$f(z) = \frac{e^z-e^{-z}}{e^z+e^{-z}}$，输出范围(-1,1)
- Softmax：多分类概率输出

## 2. 网络结构

### 2.1 基本结构
- 输入层：接收原始数据
- 隐藏层：特征提取与转换
- 输出层：产生预测结果

### 2.2 常见网络类型
- 感知机：最简单线性分类器
- 多层感知机(MLP)：全连接层堆叠
- 卷积神经网络(CNN)：擅长图像处理
- 循环神经网络(RNN)：处理序列数据
- Transformer：基于自注意力机制

## 3. 训练过程

### 3.1 前向传播
- 逐层计算：$z = Wx + b$，$a = f(z)$

### 3.2 损失函数
- 均方误差(MSE)：回归问题
- 交叉熵损失：分类问题

### 3.3 反向传播
- 计算损失对参数的梯度
- 使用链式法则

### 3.4 优化算法
- 梯度下降（批量/随机/小批量）
- 自适应优化（Adam、RMSprop等）

## 4. 实践技巧

### 4.1 数据预处理
- 归一化/标准化
- 数据增强
- 类别不平衡处理

### 4.2 正则化
- L1/L2正则化
- Dropout
- 早停法

### 4.3 超参数调优
- 学习率
- 批大小
- 网络深度与宽度

## 5. 应用领域
- 图像识别与计算机视觉
- 自然语言处理
- 语音识别
- 推荐系统
- 强化学习

## 6. 代码示例(PyTorch)

```python
import torch
import torch.nn as nn
import torch.optim as optim

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(10, 50)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(50, 1)
        
    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x

net = Net()
criterion = nn.MSELoss()
optimizer = optim.Adam(net.parameters(), lr=0.001)

for epoch in range(100):
    optimizer.zero_grad()
    outputs = net(torch.randn(32, 10))
    loss = criterion(outputs, torch.randn(32, 1))
    loss.backward()
    optimizer.step()
```

## 7. 学习资源

- 《深度学习》(Goodfellow 等)
- 《神经网络与深度学习》(Michael Nielsen)
- 斯坦福 CS231n（计算机视觉）
- 斯坦福 CS224n（自然语言处理）
