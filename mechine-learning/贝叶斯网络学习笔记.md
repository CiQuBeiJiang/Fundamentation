# 贝叶斯网络完整学习笔记

## 1. 贝叶斯网络基础

### 1.1 定义

贝叶斯网络（Bayesian Network, BN）是一种**概率图模型**，通过有向无环图（DAG）直观表示变量间的依赖关系，并用条件概率表（CPT）量化这种依赖强度，核心是将复杂的联合概率分布分解为局部条件概率的乘积，降低计算复杂度。

### 1.2 组成部分

*   **节点（Nodes）**：对应随机变量（可离散或连续，本文以离散为主），如 “是否下雨”“是否迟到”

*   **边（Edges）**：有向边表示变量间的**直接因果依赖**，如 “下雨→迟到” 表示下雨是导致迟到的直接原因

*   **条件概率表（CPT）**：每个节点的 CPT 描述该节点在父节点所有可能取值组合下的条件概率，例如节点 “迟到” 的父节点是 “下雨”，则 CPT 需包含 P (迟到 | 下雨) 和 P (迟到 | 不下雨)

### 1.3 联合概率计算（核心公式）

#### 1.3.1 数学推导

根据**概率链规则**，对于随机变量集合 $X = \{X_1, X_2, ..., X_n\}$，其联合概率可表示为：

$P(X_1, X_2, ..., X_n) = P(X_1) \times P(X_2|X_1) \times P(X_3|X_1,X_2) \times ... \times P(X_n|X_1,X_2,...,X_{n-1})$

贝叶斯网络的**局部马尔可夫性假设**：每个节点在给定其父节点的条件下，与所有非后代节点相互独立。即对于节点 $X_i$，若其父节点集合为 $\text{Parents}(X_i)$，则：

$P(X_i | X_1, ..., X_{i-1}) = P(X_i | \text{Parents}(X_i))$

将局部马尔可夫性代入链规则，可得贝叶斯网络联合概率分解公式：

$\boxed{P(X_1, X_2, ..., X_n) = \prod_{i=1}^{n} P(X_i | \text{Parents}(X_i))}$

#### 1.3.2 示例验证

以 “地震 - 盗窃 - 警报” 网络（节点：地震 E、盗窃 B、警报 A）为例，结构为 $E \rightarrow A$、$B \rightarrow A$，父节点集合：$\text{Parents}(E)=\emptyset$、$\text{Parents}(B)=\emptyset$、$\text{Parents}(A)=\{E,B\}$。

根据公式，联合概率 $P(E,B,A)$ 分解为：

$P(E,B,A) = P(E) \times P(B) \times P(A|E,B)$

若已知：$P(E)=0.001$、$P(B)=0.01$、$P(A|E=1,B=1)=0.95$（1 表示 “发生”，0 表示 “不发生”），则：

$P(E=1,B=1,A=1) = 0.001 \times 0.01 \times 0.95 = 9.5 \times 10^{-6}$

## 2. 贝叶斯网络的构建（结构 + 参数学习）

### 2.1 结构学习

结构学习的目标是从数据中挖掘变量间的依赖关系，生成最优 DAG。

#### 2.1.1 基于约束的方法（以 PC 算法为例）

**核心思想**：通过条件独立性检验（如卡方检验、互信息检验）删除变量间的冗余边，保留显著依赖关系。

**步骤推导**：

1.  **初始化**：构建完全无向图（所有变量间均有边）；

2.  **独立性检验**：对每个变量对 $(X,Y)$，在给定不同的条件集 $S$ 下，检验 $X \perp Y | S$（条件独立），若成立则删除边 $(X,Y)$；

3.  **定向边**：通过 “V 结构”（如 $X-Y-Z$ 中，$X$ 与 $Z$ 条件独立，但 $X$、$Z$ 均与 $Y$ 依赖）确定边的方向，避免形成环。

#### 2.1.2 基于评分的方法（以 BIC 评分为例）

**核心思想**：定义评分函数量化 “结构 - 数据” 的拟合度，搜索最优结构使评分最高。

**BIC 评分公式推导**：

BIC（Bayesian Information Criterion）评分平衡模型拟合度与复杂度，公式为：

$\text{BIC}(G,D) = \log P(D|G,\hat{\theta}_G) - \frac{1}{2} \log n \times \text{dim}(G)$

*   $G$：贝叶斯网络结构；$D$：数据集；$n$：数据样本量；

*   $\log P(D|G,\hat{\theta}_G)$：似然项，衡量结构 $G$ 与参数 $\hat{\theta}_G$ 对数据的拟合度，$\hat{\theta}_G$ 是参数的最大似然估计；

*   $\frac{1}{2} \log n \times \text{dim}(G)$：惩罚项，$\text{dim}(G)$ 是模型参数个数（所有 CPT 中概率值的数量，如节点 $X_i$ 有 $k$ 个父节点，每个父节点有 $m_j$ 个取值，则 $\text{dim}(X_i) = (m_1 \times ... \times m_k) \times (v_i - 1)$，$v_i$ 是 $X_i$ 的取值数）。

**评分规则**：对不同结构计算 BIC 评分，选择评分最高的结构。

### 2.2 参数学习

参数学习是在已知结构的前提下，从数据中估计 CPT 中的概率值。

#### 2.2.1 最大似然估计（MLE）

**适用场景**：数据充足，无先验知识。

**公式推导**：

设数据集 $D = \{d_1, d_2, ..., d_n\}$，每个样本 $d_i$ 是变量 $X$ 的一次取值。对于节点 $X_i$，父节点 $\text{Parents}(X_i) = \mathbf{U}$，记 $\mathbf{u}$ 是 $\mathbf{U}$ 的一个取值组合，$x_i$ 是 $X_i$ 的一个取值。

定义**计数统计**：

*   $N(\mathbf{u})$：样本中 $\mathbf{U} = \mathbf{u}$ 的数量；

*   $N(x_i, \mathbf{u})$：样本中 $X_i = x_i$ 且 $\mathbf{U} = \mathbf{u}$ 的数量。

则条件概率 $P(X_i = x_i | \mathbf{U} = \mathbf{u})$ 的最大似然估计为：

$\hat{P}(x_i | \mathbf{u}) = \frac{N(x_i, \mathbf{u})}{N(\mathbf{u})}$

**示例**：若样本中 “父节点 U=u” 出现 100 次，其中 “X\_i=x\_i 且 U=u” 出现 60 次，则 $\hat{P}(x_i | \mathbf{u}) = 60/100 = 0.6$。

#### 2.2.2 贝叶斯估计（Dirichlet 先验）

**适用场景**：数据稀疏，需引入先验知识。

**公式推导**：

1.  **先验分布**：对每个父节点取值组合 $\mathbf{u}$，假设 $P(X_i | \mathbf{u})$ 服从 Dirichlet 分布 $\text{Dir}(\alpha_{x_i|\mathbf{u}})$，其中 $\alpha_{x_i|\mathbf{u}}$ 是先验计数（如$\alpha=1$表示无信息先验，即均匀分布）；

2.  **后验分布**：结合数据后，后验计数为 $\alpha_{x_i|\mathbf{u}} + N(x_i, \mathbf{u})$，后验分布仍为 Dirichlet 分布 $\text{Dir}(\alpha_{x_i|\mathbf{u}} + N(x_i, \mathbf{u}))$；

3.  **参数估计**：取后验分布的均值作为估计值：

    $\hat{P}(x_i | \mathbf{u}) = \frac{\alpha_{x_i|\mathbf{u}} + N(x_i, \mathbf{u})}{\sum_{x_i'} (\alpha_{x_i'|\mathbf{u}} + N(x_i', \mathbf{u}))} = \frac{\alpha_{x_i|\mathbf{u}} + N(x_i, \mathbf{u})}{\alpha_{\mathbf{u}} + N(\mathbf{u})}$

    其中 $\alpha_{\mathbf{u}} = \sum_{x_i'} \alpha_{x_i'|\mathbf{u}}$（如$\alpha_{\mathbf{u}}=v_i$，$v_i$是$X_i$的取值数）。

**示例**：若先验计数 $\alpha_{x_i|\mathbf{u}}=1$，数据中 $N(x_i, \mathbf{u})=60$，$N(\mathbf{u})=100$，则 $\hat{P}(x_i | \mathbf{u}) = (1+60)/(v_i + 100)$，若 $v_i=2$，则结果为 $61/102 \approx 0.598$。

## 3. 贝叶斯网络推理（含数学推导）

推理是在已知部分变量取值（证据）的情况下，计算目标变量的后验概率。

### 3.1 推理类型

*   **因果推理**：从 “原因” 推 “结果”，如已知 “下雨”，求 “迟到” 的概率 $P(\text{迟到}|\text{下雨})$；

*   **诊断推理**：从 “结果” 推 “原因”，如已知 “迟到”，求 “下雨” 的概率 $P(\text{下雨}|\text{迟到})$；

*   **混合推理**：结合因果与诊断，如已知 “下雨” 和 “堵车”，求 “迟到” 的概率 $P(\text{迟到}|\text{下雨},\text{堵车})$。

### 3.2 精确推理（以变量消除法为例）

#### 3.2.1 核心思想

通过 “消除无关变量” 简化联合概率求和过程，避免直接计算高维联合概率。

#### 3.2.2 数学推导（以 “E-B-A-J-M” 网络为例）

网络结构：$E \rightarrow A$、$B \rightarrow A$、$A \rightarrow J$、$A \rightarrow M$，目标：已知 $J=1$（John 打电话）和 $M=1$（Mary 打电话），求 $P(B=1|J=1,M=1)$（盗窃发生的后验概率）。

**步骤 1：写出目标概率的贝叶斯公式**

$P(B=1|J=1,M=1) = \frac{P(B=1,J=1,M=1)}{P(J=1,M=1)}$

**步骤 2：分解分子（联合概率）**

根据贝叶斯网络联合概率公式，分子可分解为：

$P(B=1,J=1,M=1) = \sum_{E,A} P(E)P(B=1)P(A|E,B=1)P(J=1|A)P(M=1|A)$

（注：对无关变量 $E$ 和 $A$ 求和，即 “边际化”）

**步骤 3：变量消除（先消除 E，再消除 A）**

*   消除 $E$：对 $E$ 的所有取值（0 和 1）求和：

    $\sum_{E} P(E)P(A|E,B=1) = P(E=0)P(A|E=0,B=1) + P(E=1)P(A|E=1,B=1)$

    记为 $f(A,B=1)$（仅与 $A$ 和 $B=1$ 相关的函数）。

*   代入分子并消除 $A$：

    $P(B=1,J=1,M=1) = P(B=1) \times \sum_{A} f(A,B=1) \times P(J=1|A) \times P(M=1|A)$

**步骤 4：计算分母（证据的边际概率）**

分母 $P(J=1,M=1)$ 是所有 $B$ 取值的边际概率之和：

$P(J=1,M=1) = P(B=1,J=1,M=1) + P(B=0,J=1,M=1)$

（$P(B=0,J=1,M=1)$ 的计算过程与分子类似，仅需将 $B=1$ 替换为 $B=0$）

**步骤 5：代入数值计算**

已知参数：

*   $P(E=0)=0.999$，$P(E=1)=0.001$；$P(B=0)=0.99$，$P(B=1)=0.01$；

*   $P(A|E=0,B=0)=0.001$，$P(A|E=0,B=1)=0.9$，$P(A|E=1,B=0)=0.9$，$P(A|E=1,B=1)=0.95$；

*   $P(J=1|A=0)=0.05$，$P(J=1|A=1)=0.9$；$P(M=1|A=0)=0.01$，$P(M=1|A=1)=0.7$。

代入计算得：

*   $P(B=1,J=1,M=1) \approx 0.01 \times [ (0.999 \times 0.9 + 0.001 \times 0.95) \times 0.9 \times 0.7 + (0.999 \times 0.1 + 0.001 \times 0.05) \times 0.05 \times 0.01 ] \approx 0.00567$

*   $P(B=0,J=1,M=1) \approx 0.99 \times [ (0.999 \times 0.001 + 0.001 \times 0.9) \times 0.9 \times 0.7 + ... ] \approx 0.00062$

*   最终 $P(B=1|J=1,M=1) \approx 0.00567 / (0.00567 + 0.00062) \approx 0.90$。

### 3.3 近似推理（以 MCMC 采样为例）

#### 3.3.1 核心思想

通过大量采样生成符合网络联合分布的样本，用样本中 “目标事件出现的频率” 近似后验概率。

#### 3.3.2 关键步骤（Gibbs 采样）

1.  **初始化**：给所有变量赋值（如随机初始化 $E=0,B=0,A=0,J=0,M=0$）；

2.  **固定证据**：将证据变量（如 $J=1,M=1$）的取值固定，不参与更新；

3.  **迭代采样**：对每个非证据变量（如 $E,A,B$），根据其马尔可夫毯（父节点 + 子节点 + 子节点的父节点）的当前取值，采样新值：

*   例如更新 $A$ 时，采样分布为 $P(A|E,B,J=1,M=1) \propto P(A|E,B)P(J=1|A)P(M=1|A)$；

1.  **burn-in 期**：丢弃前 $k$ 个样本（消除初始化影响）；

2.  **计算频率**：用剩余样本中 “$B=1$” 的次数除以总样本数，得到 $P(B=1|J=1,M=1)$ 的近似值。

## 4. Python 代码实现（基于 pgmpy 库）

### 4.1 环境准备

安装 pgmpy（贝叶斯网络专用库）：

```
pip install pgmpy pandas numpy matplotlib
```

### 4.2 完整代码示例（以 “地震 - 盗窃 - 警报” 网络为例）

#### 4.2.1 1. 构建贝叶斯网络结构

```python
from pgmpy.models import BayesianNetwork

from pgmpy.factors.discrete import TabularCPD

from pgmpy.inference import VariableElimination

\# 1. 定义网络结构（节点与边）

model = BayesianNetwork(\[

&#x20;   ('Earthquake', 'Alarm'),  # E→A

&#x20;   ('Burglary', 'Alarm'),    # B→A

&#x20;   ('Alarm', 'JohnCall'),    # A→J

&#x20;   ('Alarm', 'MaryCall')     # A→M

])

\# 2. 定义条件概率表（CPT）

\# 2.1 根节点：Earthquake（无父节点）

cpd\_earthquake = TabularCPD(

&#x20;   variable='Earthquake',  # 变量名

&#x20;   variable\_card=2,        # 变量取值数（0=不发生，1=发生）

&#x20;   values=\[\[0.999], \[\</doubaocanvas>
```
