# 支持向量机（SVM）原理详解

支持向量机（Support Vector Machine，简称 SVM）是一种经典的**监督学习算法**，核心目标是在特征空间中找到一个 “最优超平面”，用于对数据进行分类（二分类为基础，可扩展至多分类），同时能有效处理高维数据和小样本场景，在文本分类、图像识别等领域有广泛应用。

## 一、SVM 的核心思想：找到 “最优超平面”

要理解 SVM，首先需要明确 “超平面” 和 “最优” 的定义 —— 这是 SVM 的逻辑起点。

### 1. 超平面：分类的 “边界”

在不同维度的特征空间中，“超平面” 是分隔两类数据的边界：

*   **2 维空间**（如特征为 “身高” 和 “体重”）：超平面是一条直线（方程：$wx + b = 0$，$w$是权重向量，$b$是偏置）；

*   **3 维空间**：超平面是一个平面（方程：$w_1x_1 + w_2x_2 + w_3x_3 + b = 0$）；

*   **n 维空间**（高维特征，如文本的词向量）：超平面是$(n-1)$维的线性子空间（方程：$w \cdot x + b = 0$，“$\cdot$” 表示向量内积）。

超平面的作用是 “分隔数据”：对于任意样本$x$，若$w \cdot x + b \geq 0$，则判定为正类（如 “合格产品”）；若$w \cdot x + b < 0$，则判定为负类（如 “不合格产品”）。

### 2. 为什么需要 “最优” 超平面？

同一组可分数据（“线性可分” 数据）可能存在无数个超平面，例如 2 维空间中两类点可以被多条直线分隔。但并非所有超平面都 “可靠”—— 有些超平面离样本太近，遇到新数据时容易分类错误（泛化能力差）。

SVM 的核心追求是：找到 \*\*“最大间隔超平面”\*\*（Maximal Margin Hyperplane），即 “离两类样本中最近点的距离最大” 的超平面。

*   这里的 “最近点” 称为**支持向量**（Support Vector）—— 它们是决定超平面位置的关键样本（移除非支持向量不影响超平面）；

*   这里的 “距离” 称为**间隔**（Margin）—— 间隔越大，模型对噪声的容忍度越高，泛化能力越强。

## 二、线性可分情况下的 SVM：数学推导

当数据在特征空间中可以被某条超平面完全分隔（无错分样本）时，称为 “线性可分”，此时 SVM 的目标是最大化间隔。

![image-20250921105425856](C:\Users\17249\AppData\Roaming\Typora\typora-user-images\image-20250921105425856.png)

### 1. 间隔的数学定义

首先需要量化 “超平面到样本的距离”：

对于超平面$w \cdot x + b = 0$，任意样本$x_i$到超平面的距离公式为：$d_i = \frac{|w \cdot x_i + b|}{\|w\|}$

其中$\|w\|$是向量$w$的 L2 范数（即$w$的长度，$\|w\| = \sqrt{w_1^2 + w_2^2 + ... + w_n^2}$）。

由于超平面能正确分类，对于正类样本（标签$y_i=1$），有$w \cdot x_i + b \geq 0$；对于负类样本（标签$y_i=-1$），有$w \cdot x_i + b \leq 0$。因此可统一表示为：

$$y_i(w \cdot x_i + b) \geq 1$$

（这里将 “$\geq 0$” 强化为 “$\geq 1$”，是为了简化计算 —— 因为超平面的参数$w$和$b$可缩放，不影响间隔的最大化目标）。

此时，**支持向量**（离超平面最近的样本）满足$y_i(w \cdot x_i + b) = 1$，这些支持向量到超平面的距离为：

$d = \frac{1}{\|w\|}$

而 SVM 的 “间隔” 是**两类支持向量之间的距离**，即$2d = \frac{2}{\|w\|}$。

### 2. 最大化间隔：转化为优化问题

SVM 的目标是 “最大化间隔”，即最大化$\frac{2}{\|w\|}$。由于$\frac{2}{\|w\|}$最大化等价于$\|w\|^2$最小化（分母越小，整体越大），因此 SVM 的优化目标可转化为：

*   **目标函数**（最小化）：$\frac{1}{2}\|w\|^2$（乘$\frac{1}{2}$是为了求导时简化计算，不影响最优解）；

*   **约束条件**（满足）：$y_i(w \cdot x_i + b) \geq 1$（对所有样本$i$）。

这是一个**带约束的凸二次规划问题**（目标函数是凸函数，约束条件是线性的），有唯一最优解。

### 3. 求解优化问题：拉格朗日乘数法

直接求解上述带约束的优化问题较复杂，SVM 通过 “拉格朗日乘数法” 将其转化为**对偶问题**（更易求解，且能自然引入核函数处理非线性问题）。

#### 步骤 1：构造拉格朗日函数

为每个约束条件$y_i(w \cdot x_i + b) \geq 1$引入非负拉格朗日乘数$\alpha_i \geq 0$，构造拉格朗日函数：

$L(w, b, \alpha) = \frac{1}{2}\|w\|^2 - \sum_{i=1}^N \alpha_i [y_i(w \cdot x_i + b) - 1]$

其中$N$是样本总数，$\alpha = (\alpha_1, \alpha_2, ..., \alpha_N)^T$是拉格朗日乘数向量。

#### 步骤 2：求对偶问题

根据凸优化的 “KKT 条件”（Karush-Kuhn-Tucker Conditions），原问题的最优解需满足以下条件：

1.  对$w$求偏导并令其为 0：$\frac{\partial L}{\partial w} = w - \sum_{i=1}^N \alpha_i y_i x_i = 0 \implies w = \sum_{i=1}^N \alpha_i y_i x_i$；

2.  对$b$求偏导并令其为 0：$\frac{\partial L}{\partial b} = -\sum_{i=1}^N \alpha_i y_i = 0 \implies \sum_{i=1}^N \alpha_i y_i = 0$；

3.  互补松弛条件：$\alpha_i [y_i(w \cdot x_i + b) - 1] = 0$（对所有$i$）。

将$w = \sum_{i=1}^N \alpha_i y_i x_i$代入拉格朗日函数，消去$w$和$b$，得到**对偶问题**：



*   **目标函数**（最大化）：$L(\alpha) = \sum_{i=1}^N \alpha_i - \frac{1}{2}\sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j y_i y_j (x_i \cdot x_j)$；

*   **约束条件**：$\sum_{i=1}^N \alpha_i y_i = 0$ 且 $\alpha_i \geq 0$（对所有$i$）。

#### 步骤 3：求解对偶问题的关键结论

对偶问题的最优解$\alpha^* = (\alpha_1^*, \alpha_2^*, ..., \alpha_N^*)^T$有一个核心性质：

大部分$ \alpha_i^*$ = 0，仅支持向量对应的 **$ \alpha_i^*>0$** 。

这意味着：最终的超平面仅由支持向量决定，非支持向量对超平面无影响 —— 这也是 SVM“抗噪声、泛化能力强” 的重要原因。

### 4. 最终的分类决策函数

通过对偶问题得到$\alpha^*$后，可还原出最优超平面的参数：

*   $w^* = \sum_{i=1}^N \alpha_i^* y_i x_i$（仅支持向量贡献非零值）；

*   $b^*$（由支持向量满足$y_i(w^* \cdot x_i + b^*) = 1$求解，取任意支持向量代入即可）。

最终的分类决策函数为：

$f(x) = sign\left( w^* \cdot x + b^* \right) = sign\left( \sum_{i=1}^N \alpha_i^* y_i (x_i \cdot x) + b^* \right)$

其中$sign(\cdot)$是符号函数：输入为正则输出 1（正类），输入为负则输出 - 1（负类）。

---



## 三、线性不可分情况：引入软间隔与松弛变量

现实中的数据往往存在 “噪声” 或 “重叠”，无法用线性超平面完全分隔（即 “线性不可分”）。此时需要放松 SVM 的约束条件，允许少量样本被错分 —— 这就是 “软间隔”（Soft Margin）SVM。

### 1. 引入松弛变量$\xi_i$

为每个样本$x_i$引入非负松弛变量$\xi_i \geq 0$，表示 “该样本允许被错分的程度”：



*   若$\xi_i = 0$：样本被正确分类，且在间隔外侧；

*   若$0 < \xi_i < 1$：样本被正确分类，但在间隔内部；

*   若$\xi_i \geq 1$：样本被错分。

此时，约束条件从 “$y_i(w \cdot x_i + b) \geq 1$” 放松为：

$y_i(w \cdot x_i + b) \geq 1 - \xi_i$

### 2. 调整目标函数：平衡间隔与错分代价

软间隔 SVM 的目标是 “在最大化间隔的同时，最小化错分样本的数量（即$\sum \xi_i$）”。因此目标函数调整为：



*   **目标函数**（最小化）：$\frac{1}{2}\|w\|^2 + C \sum_{i=1}^N \xi_i$；

*   **约束条件**：$y_i(w \cdot x_i + b) \geq 1 - \xi_i$ 且 $\xi_i \geq 0$（对所有$i$）。

其中$C > 0$是**惩罚参数**，控制 “最大化间隔” 与 “最小化错分代价” 的权衡：



*   $C$越大：对错误的惩罚越重，模型更倾向于减少错分，但可能导致间隔变小（过拟合风险高）；

*   $C$越小：对错误的惩罚越轻，模型更倾向于增大间隔，但可能导致错分增多（欠拟合风险高）。

### 3. 软间隔的对偶问题

与线性可分情况类似，软间隔 SVM 的对偶问题仅约束条件有细微变化：



*   **目标函数**（最大化）：$L(\alpha) = \sum_{i=1}^N \alpha_i - \frac{1}{2}\sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j y_i y_j (x_i \cdot x_j)$；

*   **约束条件**：$\sum_{i=1}^N \alpha_i y_i = 0$ 且 $0 \leq \alpha_i \leq C$（对所有$i$）。

此时 KKT 条件的互补松弛条件变为：$\alpha_i [y_i(w \cdot x_i + b) - (1 - \xi_i)] = 0$ 且 $(C - \alpha_i)\xi_i = 0$，仍能保证 “仅支持向量的$\alpha_i > 0$”。

## 四、非线性问题：引入核函数（Kernel Trick）

当数据在原始特征空间中无法用线性超平面分隔时（如 “环形分布” 的样本），SVM 的解决方案是：**将原始特征空间映射到更高维的特征空间**，使数据在高维空间中线性可分 —— 而核函数是实现这一映射的 “高效工具”。

### 1. 为什么需要核函数？

假设原始特征空间是 2 维（$x = (x_1, x_2)$），数据是环形分布（如$x_1^2 + x_2^2 = 1$内侧为负类，外侧为正类）。此时原始空间中无法用直线分隔，但映射到 3 维空间（$z = (x_1^2, x_2^2, \sqrt{2}x_1x_2)$）后，环形数据会转化为线性可分的平面（如$z_1 + z_2 = 1$）。

但直接进行高维映射存在一个问题：**维度灾难**—— 若原始维度为$n$，映射到$k$维（$k \gg n$），计算量会急剧增加（尤其是计算$x_i \cdot x_j$时，需先映射再内积）。

核函数的核心作用是：**无需显式进行高维映射，直接在原始空间中计算 “高维空间中两个样本的内积”**，即：

$K(x_i, x_j) = \phi(x_i) \cdot \phi(x_j)$

其中$\phi(x)$是原始空间到高维空间的映射函数，$K(x_i, x_j)$是核函数。

### 2. 常用的核函数

SVM 中常用的核函数有以下 3 类，需根据数据的分布特点选择：

| 核函数类型                   | 数学表达式                                        | 适用场景                             |
| ----------------------- | -------------------------------------------- | -------------------------------- |
| 线性核（Linear Kernel）      | $K(x_i, x_j) = x_i \cdot x_j + b$            | 数据本身线性可分，或特征维度高（如文本分类，词向量维度高）    |
| 多项式核（Polynomial Kernel） | $K(x_i, x_j) = (\gamma x_i \cdot x_j + r)^d$ | 数据有明显的多项式分布特征（如图像局部特征）           |
| 高斯核（RBF Kernel，径向基核）    | $K(x_i, x_j) = exp(-\gamma \|x_i - x_j\|^2)$ | 数据分布复杂，无法用线性或多项式核描述（如非线性分类、图像识别） |

其中$\gamma$（核系数）、$r$（常数项）、$d$（多项式次数）是核函数的超参数，需通过交叉验证（如网格搜索）调优。

### 3. 核函数下的 SVM 决策函数

将核函数代入对偶问题的目标函数和决策函数，即可得到非线性 SVM 的决策函数：

$f(x) = sign\left( \sum_{i=1}^N \alpha_i^* y_i K(x_i, x) + b^* \right)$

此时，超平面是高维空间中的线性超平面，对应原始空间中的非线性边界（如曲线、曲面）。

## 五、SVM 的优缺点总结

### 优点

1.  **泛化能力强**：基于最大间隔原则，对噪声不敏感，在小样本、高维数据场景下表现优异（如文本分类）；

2.  **抗过拟合**：仅依赖支持向量，模型复杂度低，不易过拟合；

3.  **非线性处理能力**：通过核函数可灵活处理非线性数据，无需手动设计特征映射；

4.  **数学理论严谨**：基于凸优化，有唯一最优解，避免局部最优。

### 缺点

1.  **计算复杂度高**：对大规模样本（如$N > 10^5$），求解对偶问题的时间和空间复杂度高（需存储支持向量）；

2.  **超参数调优复杂**：惩罚参数$C$和核函数的超参数（如$\gamma$）需通过交叉验证调优，过程耗时；

3.  **多分类支持弱**：SVM 本质是二分类算法，扩展至多分类需通过 “一对一”（One-vs-One）或 “一对多”（One-vs-Rest）策略，效率较低。

## 六、SVM 与其他分类算法的对比

| 算法       | 核心优势          | 核心劣势           | 适用场景                   |
| -------- | ------------- | -------------- | ---------------------- |
| SVM      | 泛化能力强，高维数据表现好 | 大规模数据效率低，多分类复杂 | 小样本、高维数据（文本分类、图像识别）    |
| 逻辑回归（LR） | 训练快，可输出概率，易解释 | 非线性数据需手动特征工程   | 大规模数据、需概率输出（如风控评分）     |
| 决策树（DT）  | 可解释性强，处理非线性数据 | 易过拟合，泛化能力弱     | 小样本、需解释决策过程（如医疗诊断初步筛选） |

通过以上内容，可系统理解 SVM 的核心逻辑：从 “线性可分的最大间隔超平面” 出发，通过 “软间隔” 处理噪声，通过 “核函数” 处理非线性，最终实现高效、鲁棒的分类。

## 七、代码实现

```python
import numpy as np
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score

class BasicSVM:
    def __init__(self, kernel='linear', C=1.0, gamma='scale', degree=3):
        """
        SVM基础模型初始化，包含核心参数
        
        参数说明：
        - kernel: 核函数类型，常用值：
            'linear'：线性核（默认，适用于线性可分数据）
            'rbf'：高斯径向基核（适用于非线性数据，默认选择）
            'poly'：多项式核（适用于有多项式分布特征的数据）
        - C: 惩罚系数（正则化参数）
            值越大：对错误分类的惩罚越重，模型容易过拟合
            值越小：允许更多错误，模型容易欠拟合
            常用范围：0.001 ~ 1000，一般从1.0开始尝试
        - gamma: 核系数（仅对'rbf'、'poly'有效）
            'scale'：默认值，=1/(n_features * X.var())
            'auto'：=1/n_features
            自定义值：值越大，核函数影响范围越小（模型复杂）
        - degree: 多项式核的阶数（仅对'poly'有效）
            常用值：2~5，值越大模型越复杂
        """
        # 特征标准化器（SVM必须做特征标准化）
        self.scaler = StandardScaler()
        # 初始化SVM模型
        self.model = SVC(
            kernel=kernel,
            C=C,
            gamma=gamma,
            degree=degree,
            probability=False  # 是否启用概率估计，会增加计算量
        )
        
    def train(self, X, y):
        """训练模型"""
        # 特征标准化，标准化后的值 = (原始值 - 该特征的均值) / 该特征的标准差
        X_scaled = self.scaler.fit_transform(X)
        # 划分训练集和验证集（可选步骤）
        X_train, X_val, y_train, y_val = train_test_split(
            X_scaled, y, test_size=0.2, random_state=42
        )
        # 训练模型
        self.model.fit(X_train, y_train)
        # 验证集评估
        y_pred = self.model.predict(X_val)
        print(f"验证集准确率: {accuracy_score(y_val, y_pred):.4f}")
        return self
        
    def predict(self, X):
        """预测新数据"""
        # 对新数据做同样的标准化
        X_scaled = self.scaler.transform(X)
        return self.model.predict(X_scaled)
        
    def evaluate(self, X, y):
        """评估模型在测试集上的表现"""
        y_pred = self.predict(X)
        return accuracy_score(y, y_pred)


# ----------------------
# 使用示例：如何根据新任务修改参数
# ----------------------
if __name__ == "__main__":
    # 1. 模拟数据（实际使用时替换为你的数据）
    # 生成二维分类数据（1000样本，2特征，2类别）
    X = np.random.randn(1000, 2)
    y = np.where(X[:, 0] + X[:, 1] > 0, 1, 0)  # 简单线性可分数据
    
    # 2. 初始化模型并根据数据特点调整参数
    # 情况1：线性可分数据（如本示例）
    svm_linear = BasicSVM(
        kernel='linear',  # 线性核足够
        C=1.0             # 中等惩罚
    )
    
    # 情况2：如果数据是非线性的（比如环形分布）
    # svm_nonlinear = BasicSVM(
    #     kernel='rbf',    # 改用高斯核
    #     C=10.0,          # 适当增大惩罚
    #     gamma=0.5        # 调整核影响范围
    # )
    
    # 情况3：多项式分布数据
    # svm_poly = BasicSVM(
    #     kernel='poly',   # 多项式核
    #     degree=2,        # 2阶多项式
    #     C=5.0
    # )
    
    # 3. 训练模型
    svm_linear.train(X, y)
    
    # 4. 测试新数据
    X_test = np.random.randn(100, 2)
    y_test = np.where(X_test[:, 0] + X_test[:, 1] > 0, 1, 0)
    test_acc = svm_linear.evaluate(X_test, y_test)
    print(f"测试集准确率: {test_acc:.4f}")

```



